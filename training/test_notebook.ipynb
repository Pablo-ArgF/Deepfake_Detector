{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D,MaxPooling3D, Conv3D, Flatten,Lambda, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from faceRecon import FaceExtractorMultithread, FaceExtractor\n",
    "\n",
    "route = '/home/pabloarga/Data'\n",
    "\n",
    "print('Loading dataframes...')\n",
    "fragments = []\n",
    "for i in range(5):\n",
    "    # Assuming each dataframe is stored as a separate table in the HDF5 file\n",
    "    if i%10==0:\n",
    "        print(f'{i} dataframes loaded')\n",
    "    chunk = pd.read_hdf(f'{route}/dataframe{i}_FaceForensics.h5', key=f'df{i}')\n",
    "    fragments.append(chunk)\n",
    "#fragments = [pd.read_hdf(f'dataframes/CelebDB/dataframe{i}_600videos.h5', key=f'df{i}') for i in range(2)]#6\n",
    "df = pd.concat(fragments)\n",
    "\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "X = df.drop(['label'], axis = 1)\n",
    "y = df['label']\n",
    "\n",
    "print('Dividing dataset into train and test...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify = y)\n",
    "X_train = np.stack(X_train['face'], axis=0)\n",
    "X_test = np.stack(X_test['face'], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating model...')\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(200, 200, 3)))\n",
    "model.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetricsModule import TrainingMetrics\n",
    "#prueba de que imprime las stats\n",
    "metrics = TrainingMetrics(model,\"/home/pabloarga/Results\")\n",
    "metrics.train(X_train, y_train, X_test, y_test, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de bach training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten,Lambda,  Input,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from MetricsModule import TrainingMetrics\n",
    "\n",
    "route =  'P:\\TFG\\Datasets\\dataframes_small' #'/home/pabloarga/Data' \n",
    "resultsPath = 'P:\\TFG\\Datasets\\dataframes_small\\\\results' #'/home/pabloarga/Results'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(200, 200, 3)))\n",
    "model.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Input(shape=(200, 200, 3)))\n",
    "model2.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.2))  # Dropout for regularization\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Input(shape=(200, 200, 3)))\n",
    "model3.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(0.2))  # Dropout for regularization\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dropout(0.3))  # Dropout for regularization\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetricsModule import TrainingMetrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Congelar las capas del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crear un nuevo modelo encima del modelo base\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "metrics = TrainingMetrics(model, resultsPath)\n",
    "metrics.batches_train(route,nBatches = 1 , epochs = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos el modelo y probamos con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from FaceReconModule import FaceExtractorMultithread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "modelPath = os.path.join(resultsPath,'model.keras')\n",
    "\n",
    "#cargamos un video y lo procesamos frame por frame\n",
    "videoPath = 'P:\\TFG\\Datasets\\FaceForensics\\manipulated_sequences-fake\\DeepFakeDetection\\c23\\\\videos\\\\01_15__outside_talking_pan_laughing__02HILKYO.mp4'\n",
    "#creamos un dataframe con el path al video y con la label 0 de fake\n",
    "df = pd.DataFrame({'video': [videoPath], 'label': [0]})\n",
    "faceExtractor = FaceExtractorMultithread(400) #cada 2 frames\n",
    "imagesDataset = faceExtractor.transform(df)\n",
    "\n",
    "print(imagesDataset['face'])\n",
    "\n",
    "#Cargamos el modelo\n",
    "model = load_model(modelPath,safe_mode=False)\n",
    "#probamos el modelo manualmente\n",
    "y_pred = model.predict(np.stack(imagesDataset['face'], axis=0))\n",
    "#matriz de confusión\n",
    "y_real = imagesDataset['label']\n",
    "print(confusion_matrix(y_real, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FaceReconModule'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFaceReconModule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceExtractorMultithread, FaceExtractor\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mTODO comentar\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment\u001b[39m(row):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'FaceReconModule'"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D,MaxPooling3D, Conv3D, Flatten,Lambda, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from FaceReconModule import FaceExtractorMultithread, FaceExtractor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TODO comentar\n",
    "\"\"\"\n",
    "def augment(row):\n",
    "    image = np.array(row['face']).reshape((200, 200, 3))  # Replace height and width with the dimensions of your images\n",
    "\n",
    "    # Perform augmentations only on fake images \n",
    "    if row['label'] == 1:\n",
    "        # Flip the image\n",
    "        flipped_image = np.fliplr(image)\n",
    "        # Rotate the image\n",
    "        rotated_image = ndimage.rotate(image, 15)  # Adjust the angle as needed\n",
    "\n",
    "        # Add the augmented images as new examples\n",
    "        new_row_flipped = row.copy()\n",
    "        new_row_flipped['face'] = flipped_image.flatten().tolist()\n",
    "        new_row_flipped['label'] = 1\n",
    "        new_row_rotated = row.copy()\n",
    "        new_row_rotated['face'] = rotated_image.flatten().tolist()\n",
    "        new_row_rotated['label'] = 1\n",
    "\n",
    "        return pd.DataFrame([row, new_row_flipped, new_row_rotated])\n",
    "\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folderPath = 'C:\\\\Users\\\\pablo\\\\Desktop\\\\TFG (1)\\\\test_datasets\\\\dataframes'\n",
    "\n",
    "nBatches = 10\n",
    "\n",
    "numDataframes = len([name for name in os.listdir(folderPath) if os.path.isfile(os.path.join(folderPath, name))])\n",
    "#Calculamos el tamaño de cada fragmento\n",
    "fragmentSize = int(numDataframes/nBatches)\n",
    "\n",
    "for i in range(nBatches):\n",
    "    fragments = [pd.read_hdf(f'{folderPath}/dataframe{j}_FaceForensics.h5', key=f'df{j}') for j in range(fragmentSize*i,fragmentSize*(i+1))]\n",
    "    df = pd.concat(fragments)\n",
    "    #contamos el número de fakes y reales en el dataframe\n",
    "    print(df['label'].value_counts())\n",
    "    #print(\"After augmentation\")\n",
    "    #df = pd.concat(df.apply(augment, axis=1).tolist(), ignore_index=True)\n",
    "    #print(df['label'].value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "#cargamos el modelo del escritorio\n",
    "modelPath = '/home/pabloarga/Results/2024-03-04 14.39.53/model2024-03-04 14.39.53.keras'\n",
    "videoTest = '/home/pabloarga/testVideos/testVideo.mp4'\n",
    "\n",
    "#Cargamos el modelo\n",
    "model = load_model(modelPath,safe_mode=False,compile=False)\n",
    "#obtenemos el número de frames del video\n",
    "cap = cv2.VideoCapture(videoTest)\n",
    "nFramesVideo = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#Procesamos el video frame por frame\n",
    "faceExtractor = FaceExtractorMultithread(nFramesVideo) #cada 2 frames\n",
    "frames = faceExtractor.process_video(videoTest,1)[0]\n",
    "\n",
    "#probamos el modelo manualmente\n",
    "y_pred = model.predict(np.stack(frames, axis=0))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 18:56:51.706437: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingMetrics' object has no attribute 'storeTrainingScript'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m resultsPathServer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/pabloarga/MiniData\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m     76\u001b[0m metrics \u001b[38;5;241m=\u001b[39m TrainingMetrics(model, resultsPathServer, modelDescription \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouteServer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnBatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Divide the whole dataset into <nbatches> fragments and train <epochs> epochs with each\u001b[39;00m\n",
      "File \u001b[0;32m~/Deepfake_Detector/training/MetricsModule.py:102\u001b[0m, in \u001b[0;36mTrainingMetrics.batches_train\u001b[0;34m(self, folderPath, nBatches, epochs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatches_train\u001b[39m(\u001b[38;5;28mself\u001b[39m,folderPath,nBatches,epochs):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#Guardamos una copia del archivo de entrenamiento en la carpeta del modelo\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoreTrainingScript\u001b[49m()\n\u001b[1;32m    104\u001b[0m     fileNames \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folderPath) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folderPath, name))]\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m#Hacemos un shuffle a los archivos para mezclar los dataframes\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingMetrics' object has no attribute 'storeTrainingScript'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Lambda, Input, Dropout, PReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from MetricsModule import TrainingMetrics\n",
    "\n",
    "# Define the constant value for PReLU alpha\n",
    "value_PReLU = 0.25\n",
    "\n",
    "# Input Layer\n",
    "inputs = layers.Input(shape=(200, 200, 3))\n",
    "\n",
    "# Function to add Convolutional layer with PReLU activation\n",
    "def conv_prelu(filters, kernel_size, name):\n",
    "    conv_layer = layers.Conv2D(filters, kernel_size, padding='same', name=name)\n",
    "    prelu_layer = PReLU(alpha_initializer=Constant(value=value_PReLU))\n",
    "    return Sequential([conv_layer, prelu_layer])\n",
    "\n",
    "# Conv1_1 and Conv1_2 Layers\n",
    "x = conv_prelu(64, (3, 3), 'conv1_1')(inputs)\n",
    "x = conv_prelu(64, (3, 3), 'conv1_2')(x)\n",
    "x = layers.Dropout(0.25)(x)  # Adding dropout after Conv1_2\n",
    "\n",
    "# Pool1 Layer\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "# Conv2_1 and Conv2_2 Layers\n",
    "x = conv_prelu(128, (3, 3), 'conv2_1')(x)\n",
    "x = conv_prelu(128, (3, 3), 'conv2_2')(x)\n",
    "x = layers.Dropout(0.25)(x)  # Adding dropout after Conv2_2\n",
    "\n",
    "# Pool2 Layer\n",
    "pool2_output = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n",
    "\n",
    "# Now you can use pool2_output as input for other layers\n",
    "conv3_1 = conv_prelu(128, (3, 3), 'conv3_1')(pool2_output)\n",
    "conv3_2 = conv_prelu(128, (3, 3), 'conv3_2')(conv3_1)\n",
    "pool3 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_2)\n",
    "conv4_1 = conv_prelu(128, (3, 3), 'conv4_1')(pool3)\n",
    "conv4_2 = conv_prelu(128, (3, 3), 'conv4_2')(conv4_1)\n",
    "\n",
    "conv5_2 = conv_prelu(128, (3, 3), 'conv5_2')(pool2_output)\n",
    "conv5_3 = conv_prelu(128, (3, 3), 'conv5_3')(conv5_2)\n",
    "\n",
    "conv5_1 = conv_prelu(128, (3, 3), 'conv5_1')(pool2_output)\n",
    "concat_1 = layers.Concatenate(name=\"concat_1\")([conv3_2, conv5_1, conv5_3])\n",
    "pool5 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool5')(concat_1)\n",
    "\n",
    "conv6_2 = conv_prelu(128, (3, 3), 'conv6_2')(pool5)\n",
    "conv6_3 = conv_prelu(128, (3, 3), 'conv6_3')(conv6_2)\n",
    "\n",
    "conv6_1 = conv_prelu(128, (3, 3), 'conv6_1')(pool5)\n",
    "concat_2 = layers.Concatenate(name=\"concat_2\")([conv4_2, conv6_1, conv6_3])\n",
    "\n",
    "pool4 = layers.MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(concat_2)\n",
    "\n",
    "flatten = Flatten()(pool4)\n",
    "\n",
    "#fc = layers.Dense(1024, name='fc')(flatten)\n",
    "#fc = layers.Dropout(0.5)(fc)  # Adding dropout before the fully connected layer\n",
    "\n",
    "#fc_class = layers.Dense(4096, name='fc_class')(fc)\n",
    "\n",
    "# Softmax Output Layer\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='out')(flatten)\n",
    "\n",
    "# Compile the model (add optimizer, loss function, etc.)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "routeServer = '/home/pabloarga/MiniData'\n",
    "resultsPathServer = '/home/pabloarga/MiniData' \n",
    "\n",
    "metrics = TrainingMetrics(model, resultsPathServer, modelDescription = 'test')\n",
    "metrics.batches_train(routeServer, nBatches=1, epochs=1)  # Divide the whole dataset into <nbatches> fragments and train <epochs> epochs with each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import base64\n",
    "import sys\n",
    "#My modules\n",
    "sys.path.append(\"..\")\n",
    "from training.DataProcessing.FaceReconModule import FaceExtractorMultithread\n",
    "\n",
    "faceExtractor = FaceExtractorMultithread() \n",
    "\n",
    "# Process the video\n",
    "video_path = '/home/pabloarga/testVideos/fakeVideo_wwe.mp4'\n",
    "videoFrames, processedFrames = faceExtractor.process_video_to_predict(video_path)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(np.stack(processedFrames, axis=0))\n",
    "predictions = [float(value) for value in predictions]\n",
    "mean = np.mean(predictions)\n",
    "\n",
    "{\n",
    "    'predictions': {\n",
    "        'data': predictions\n",
    "    },\n",
    "    'mean': mean,\n",
    "    'nFrames': len(predictions),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label\n",
      "count   16.0\n",
      "mean     1.0\n",
      "std      0.0\n",
      "min      1.0\n",
      "25%      1.0\n",
      "50%      1.0\n",
      "75%      1.0\n",
      "max      1.0\n",
      "sequences    object\n",
      "label         int64\n",
      "dtype: object\n",
      "Number of sequences: 16\n",
      "Shape of the first image in the first sequence: (198, 198, 3)\n",
      "Training set size: 12\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Testing set size: 4\n",
      "Training labels size: 12\n",
      "Testing labels size: 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 198, 196, 200, 200, 193, 200, 200, 197, 200, 200, 200, 190, 198, 199, 206, 202, 202, 200, 196, 188, 196, 203, 206, 205, 200, 202, 194, 195, 200, 200, 200, 192, 197, 200, 198, 200, 202, 203, 204, 198, 200, 200, 193, 200, 206, 193, 202, 197, 191, 188, 193, 193, 200, 200, 200, 200, 200, 68, 199, 210, 199, 197, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 193, 198, 197, 196, 194, 198, 200, 200, 185, 200, 184, 200, 190, 174, 194, 200, 200, 200, 200, 195, 193, 202, 186, 188, 192, 190, 200, 190, 191, 198, 216, 209, 211, 221, 199, 212, 192, 200, 205, 204, 112, 198, 190, 200, 200, 199, 184, 203, 190, 200, 203, 199, 197, 200, 200, 190, 192, 199, 196, 193, 200, 200, 198, 195, 200, 195, 190, 200, 199, 195, 192, 190, 195, 200, 192, 190, 187, 188, 193, 187, 196, 200, 200, 200, 200, 200, 194, 200, 200, 201, 197, 200, 201, 200, 60, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 188, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 94, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 198, 200, 200, 206, 208, 200, 197, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 194, 201, 200, 201, 200, 200, 200, 198, 223, 200, 200, 200, 200, 203, 206, 202, 197, 200, 195, 200, 200, 200, 200, 200, 200, 200, 200, 196, 200, 196, 200, 200, 202, 200, 207, 233, 206, 233, 202, 202, 187, 202, 194, 200, 200, 200, 207, 206, 202, 204, 193, 195, 200, 195, 200, 200, 202, 193, 197, 200, 196, 196, 205, 201, 204, 200, 200, 200, 194, 202, 201, 198, 193, 199, 200, 199, 192, 192, 200, 192, 196, 196, 198, 197, 199, 196, 197, 201, 194, 191, 200, 200, 200, 205, 203, 200, 200, 200, 200, 200, 200, 196, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 199, 200, 195, 200, 194, 233, 200, 198, 200, 200, 200, 200, 200, 196, 200, 196, 200, 200, 200, 200, 200, 200, 226, 200, 200, 200, 200, 200, 200, 198, 200, 215, 212, 200, 200, 204, 200, 204, 200, 200, 200, 200, 208, 200, 196, 196, 198, 200, 200, 198, 200, 205, 204, 201, 206, 233, 197, 208, 204, 211, 217, 199, 203, 200, 212, 200, 200, 200, 200, 212, 200, 200, 204, 213, 212, 218, 210, 205, 207, 200, 200, 200, 200, 201, 197, 199, 196, 200, 200, 200, 200, 193, 197, 199, 205, 200, 198, 201, 196, 200, 198, 197, 201, 200, 203, 204, 202, 210, 201, 209, 200, 200, 200, 200, 202, 209, 200, 207, 203, 203, 202, 198, 200, 200, 205, 203, 200, 200, 218, 209, 205, 208, 208, 207, 208, 207, 207, 205, 200, 196, 196, 192, 201, 200, 196, 208, 213, 200, 210, 200, 200, 200, 214, 208, 202, 199, 199, 202, 195, 205, 199, 202, 200, 195, 205, 200, 200, 200, 200, 200, 203, 200, 200, 206, 200, 206, 200, 194, 193, 190, 200, 198, 201, 199, 207, 209, 203, 203, 213, 204, 197, 204, 206, 200, 202, 199, 206, 207, 201, 194, 200, 200, 200, 199, 200, 200, 202, 200, 233, 200, 213, 217, 213, 200, 200, 200, 200, 200, 205, 233, 200, 198, 208, 200, 200, 200, 200, 190, 179, 189, 192, 200, 185, 187, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 206, 203, 200, 200, 200, 200, 200, 200, 200, 199, 200, 200, 200, 200, 198, 200, 200, 200, 200, 200, 200, 200, 200, 200, 223, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 199, 200, 200, 200, 200, 199, 200, 200, 200, 201, 200, 200, 200, 195, 200, 200, 203, 202, 198, 200, 195, 201, 201, 200, 200, 197, 200, 200, 198, 202, 201, 201, 204, 206, 200, 200, 200, 204, 200, 201, 207, 211, 206, 200, 200, 200, 213, 216, 200, 204, 206, 200, 200, 200, 218, 222, 217, 220, 220, 210, 195, 200, 200, 233, 200, 204, 200, 200, 196, 183, 202, 200, 206, 195, 194, 199, 200, 200, 207, 218, 207, 196, 201, 201, 196, 196, 200, 200, 200, 200, 204, 212, 204, 209, 200, 200, 200, 200, 203, 148, 189, 200, 216, 186, 248, 194, 196, 192, 200, 200, 200, 200, 200, 71, 206, 196, 200, 200, 199, 200, 203, 201, 200, 200, 194, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 205, 201, 200, 191, 200, 189, 187, 199, 190, 200, 200, 200, 200, 200, 200, 200, 200, 195, 207, 203, 196, 200, 207, 200, 200, 199, 200, 214, 218, 210, 207, 195, 197, 193, 200, 196, 200, 205, 233, 201, 233, 205, 201, 233, 202, 206, 203, 201, 207, 210, 202, 200, 200, 200, 233, 200, 200, 200, 200, 200, 203, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 202, 203, 200, 200, 200, 200, 196, 199, 200, 200, 200, 200, 200, 195, 200, 197, 197, 200, 200, 200, 200, 198, 201, 200, 200, 198, 198, 200, 277, 200, 200, 192, 198, 200, 200, 200, 200, 191, 200, 200, 200, 200, 197, 202, 200, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 277, 200, 200, 200, 202, 202, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 194, 199, 196, 195, 196, 202, 191, 197, 198, 202, 187, 233, 195, 217, 200, 194, 201, 200, 200, 200, 200, 200, 200, 200, 200, 200, 208, 213, 230, 225, 215, 200, 200, 200, 200, 208, 200, 197, 195, 208, 205, 205, 200, 198, 198, 198, 196, 194, 189, 194, 193, 189, 200, 248, 192, 200, 200, 218, 207, 217, 223, 221, 200, 200, 215, 200, 208, 210, 203, 215, 223, 210, 200, 200, 196, 200, 193, 190, 191, 190, 188, 194, 193, 191, 194, 190, 189, 192, 192, 199, 195, 190, 192, 195, 195, 194, 195, 192, 200, 200, 200, 200, 200, 200, 200, 200, 192, 200, 200, 200, 200, 199, 198, 200, 200, 200, 200, 198, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 202, 200, 200, 196, 195, 195, 200, 194, 201, 199, 200, 197, 421, 428, 427, 420, 200, 425, 422, 407, 401, 393, 404, 405, 415, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 406, 409, 416, 395, 200, 417, 200, 408, 419, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 411, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 415, 418, 404, 395, 402, 406, 397, 394, 400, 402, 399, 406, 416, 404, 420, 408, 406, 420, 403, 414, 409, 401, 404, 412, 200, 200, 200, 200, 425, 429, 200, 200, 200, 200, 200, 404, 416, 419, 409, 410, 400, 200, 200, 200, 417, 426, 424, 421, 413, 200, 406, 200, 418, 402, 399, 200, 200, 407, 413, 200, 404, 200, 406, 404, 200, 424, 200, 411, 200, 431, 422, 422, 419, 409, 405, 406, 200, 422, 412, 421, 200, 200, 423, 200, 420, 200, 200, 200, 410, 415, 200, 411, 415, 414, 424, 417, 424, 417, 419, 422, 424, 415, 432, 419, 402, 200, 200, 200, 418, 200, 200, 412, 409, 397, 404, 409, 410, 414, 410, 423, 200, 413, 405, 410, 415, 406, 381, 426, 437, 422, 403, 410, 429, 419, 420, 414, 433, 412, 405, 417, 200, 420, 406, 200, 200, 410, 200, 429, 200, 200, 424, 416, 419, 425, 422, 414, 410, 410, 430, 428, 417, 413, 414, 394, 398, 420, 417, 200, 422, 200, 402, 200, 200, 200, 200, 200, 200, 409, 413, 200, 200, 421, 200, 200, 200, 200, 200, 404, 403, 395, 397, 404, 200, 200, 404, 407, 407, 402, 403, 409, 393, 405, 408, 200, 408, 421, 200, 200, 422, 424, 410, 418, 392, 399, 395, 421, 396, 405, 200, 389, 420, 407, 410, 411, 408, 407, 417, 416, 200, 200, 200, 200, 200, 200, 410, 424, 405, 411, 200, 200, 407, 200, 240, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 220, 200, 235, 241, 200, 200, 240, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 248, 200, 200, 230, 228, 234, 229, 224, 227, 239, 233, 233, 200, 248, 200, 200, 200, 200, 262, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 247, 200, 200, 200, 200, 200, 200, 200, 251, 200, 247, 251, 200, 200, 200, 246, 200, 200, 200, 200, 200, 244, 242, 200, 200, 200, 200, 250, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 247, 200, 200, 244, 244, 200, 200, 200\n  y sizes: 16\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     66\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1960\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1953\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1954\u001b[0m         label,\n\u001b[0;32m   1955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1956\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1957\u001b[0m         ),\n\u001b[0;32m   1958\u001b[0m     )\n\u001b[0;32m   1959\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1960\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 198, 196, 200, 200, 193, 200, 200, 197, 200, 200, 200, 190, 198, 199, 206, 202, 202, 200, 196, 188, 196, 203, 206, 205, 200, 202, 194, 195, 200, 200, 200, 192, 197, 200, 198, 200, 202, 203, 204, 198, 200, 200, 193, 200, 206, 193, 202, 197, 191, 188, 193, 193, 200, 200, 200, 200, 200, 68, 199, 210, 199, 197, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 193, 198, 197, 196, 194, 198, 200, 200, 185, 200, 184, 200, 190, 174, 194, 200, 200, 200, 200, 195, 193, 202, 186, 188, 192, 190, 200, 190, 191, 198, 216, 209, 211, 221, 199, 212, 192, 200, 205, 204, 112, 198, 190, 200, 200, 199, 184, 203, 190, 200, 203, 199, 197, 200, 200, 190, 192, 199, 196, 193, 200, 200, 198, 195, 200, 195, 190, 200, 199, 195, 192, 190, 195, 200, 192, 190, 187, 188, 193, 187, 196, 200, 200, 200, 200, 200, 194, 200, 200, 201, 197, 200, 201, 200, 60, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 188, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 94, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 198, 200, 200, 206, 208, 200, 197, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 194, 201, 200, 201, 200, 200, 200, 198, 223, 200, 200, 200, 200, 203, 206, 202, 197, 200, 195, 200, 200, 200, 200, 200, 200, 200, 200, 196, 200, 196, 200, 200, 202, 200, 207, 233, 206, 233, 202, 202, 187, 202, 194, 200, 200, 200, 207, 206, 202, 204, 193, 195, 200, 195, 200, 200, 202, 193, 197, 200, 196, 196, 205, 201, 204, 200, 200, 200, 194, 202, 201, 198, 193, 199, 200, 199, 192, 192, 200, 192, 196, 196, 198, 197, 199, 196, 197, 201, 194, 191, 200, 200, 200, 205, 203, 200, 200, 200, 200, 200, 200, 196, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 199, 200, 195, 200, 194, 233, 200, 198, 200, 200, 200, 200, 200, 196, 200, 196, 200, 200, 200, 200, 200, 200, 226, 200, 200, 200, 200, 200, 200, 198, 200, 215, 212, 200, 200, 204, 200, 204, 200, 200, 200, 200, 208, 200, 196, 196, 198, 200, 200, 198, 200, 205, 204, 201, 206, 233, 197, 208, 204, 211, 217, 199, 203, 200, 212, 200, 200, 200, 200, 212, 200, 200, 204, 213, 212, 218, 210, 205, 207, 200, 200, 200, 200, 201, 197, 199, 196, 200, 200, 200, 200, 193, 197, 199, 205, 200, 198, 201, 196, 200, 198, 197, 201, 200, 203, 204, 202, 210, 201, 209, 200, 200, 200, 200, 202, 209, 200, 207, 203, 203, 202, 198, 200, 200, 205, 203, 200, 200, 218, 209, 205, 208, 208, 207, 208, 207, 207, 205, 200, 196, 196, 192, 201, 200, 196, 208, 213, 200, 210, 200, 200, 200, 214, 208, 202, 199, 199, 202, 195, 205, 199, 202, 200, 195, 205, 200, 200, 200, 200, 200, 203, 200, 200, 206, 200, 206, 200, 194, 193, 190, 200, 198, 201, 199, 207, 209, 203, 203, 213, 204, 197, 204, 206, 200, 202, 199, 206, 207, 201, 194, 200, 200, 200, 199, 200, 200, 202, 200, 233, 200, 213, 217, 213, 200, 200, 200, 200, 200, 205, 233, 200, 198, 208, 200, 200, 200, 200, 190, 179, 189, 192, 200, 185, 187, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 206, 203, 200, 200, 200, 200, 200, 200, 200, 199, 200, 200, 200, 200, 198, 200, 200, 200, 200, 200, 200, 200, 200, 200, 223, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 199, 200, 200, 200, 200, 199, 200, 200, 200, 201, 200, 200, 200, 195, 200, 200, 203, 202, 198, 200, 195, 201, 201, 200, 200, 197, 200, 200, 198, 202, 201, 201, 204, 206, 200, 200, 200, 204, 200, 201, 207, 211, 206, 200, 200, 200, 213, 216, 200, 204, 206, 200, 200, 200, 218, 222, 217, 220, 220, 210, 195, 200, 200, 233, 200, 204, 200, 200, 196, 183, 202, 200, 206, 195, 194, 199, 200, 200, 207, 218, 207, 196, 201, 201, 196, 196, 200, 200, 200, 200, 204, 212, 204, 209, 200, 200, 200, 200, 203, 148, 189, 200, 216, 186, 248, 194, 196, 192, 200, 200, 200, 200, 200, 71, 206, 196, 200, 200, 199, 200, 203, 201, 200, 200, 194, 194, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 205, 201, 200, 191, 200, 189, 187, 199, 190, 200, 200, 200, 200, 200, 200, 200, 200, 195, 207, 203, 196, 200, 207, 200, 200, 199, 200, 214, 218, 210, 207, 195, 197, 193, 200, 196, 200, 205, 233, 201, 233, 205, 201, 233, 202, 206, 203, 201, 207, 210, 202, 200, 200, 200, 233, 200, 200, 200, 200, 200, 203, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 202, 203, 200, 200, 200, 200, 196, 199, 200, 200, 200, 200, 200, 195, 200, 197, 197, 200, 200, 200, 200, 198, 201, 200, 200, 198, 198, 200, 277, 200, 200, 192, 198, 200, 200, 200, 200, 191, 200, 200, 200, 200, 197, 202, 200, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 277, 200, 200, 200, 202, 202, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 194, 199, 196, 195, 196, 202, 191, 197, 198, 202, 187, 233, 195, 217, 200, 194, 201, 200, 200, 200, 200, 200, 200, 200, 200, 200, 208, 213, 230, 225, 215, 200, 200, 200, 200, 208, 200, 197, 195, 208, 205, 205, 200, 198, 198, 198, 196, 194, 189, 194, 193, 189, 200, 248, 192, 200, 200, 218, 207, 217, 223, 221, 200, 200, 215, 200, 208, 210, 203, 215, 223, 210, 200, 200, 196, 200, 193, 190, 191, 190, 188, 194, 193, 191, 194, 190, 189, 192, 192, 199, 195, 190, 192, 195, 195, 194, 195, 192, 200, 200, 200, 200, 200, 200, 200, 200, 192, 200, 200, 200, 200, 199, 198, 200, 200, 200, 200, 198, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 202, 200, 200, 196, 195, 195, 200, 194, 201, 199, 200, 197, 421, 428, 427, 420, 200, 425, 422, 407, 401, 393, 404, 405, 415, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 406, 409, 416, 395, 200, 417, 200, 408, 419, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 411, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 415, 418, 404, 395, 402, 406, 397, 394, 400, 402, 399, 406, 416, 404, 420, 408, 406, 420, 403, 414, 409, 401, 404, 412, 200, 200, 200, 200, 425, 429, 200, 200, 200, 200, 200, 404, 416, 419, 409, 410, 400, 200, 200, 200, 417, 426, 424, 421, 413, 200, 406, 200, 418, 402, 399, 200, 200, 407, 413, 200, 404, 200, 406, 404, 200, 424, 200, 411, 200, 431, 422, 422, 419, 409, 405, 406, 200, 422, 412, 421, 200, 200, 423, 200, 420, 200, 200, 200, 410, 415, 200, 411, 415, 414, 424, 417, 424, 417, 419, 422, 424, 415, 432, 419, 402, 200, 200, 200, 418, 200, 200, 412, 409, 397, 404, 409, 410, 414, 410, 423, 200, 413, 405, 410, 415, 406, 381, 426, 437, 422, 403, 410, 429, 419, 420, 414, 433, 412, 405, 417, 200, 420, 406, 200, 200, 410, 200, 429, 200, 200, 424, 416, 419, 425, 422, 414, 410, 410, 430, 428, 417, 413, 414, 394, 398, 420, 417, 200, 422, 200, 402, 200, 200, 200, 200, 200, 200, 409, 413, 200, 200, 421, 200, 200, 200, 200, 200, 404, 403, 395, 397, 404, 200, 200, 404, 407, 407, 402, 403, 409, 393, 405, 408, 200, 408, 421, 200, 200, 422, 424, 410, 418, 392, 399, 395, 421, 396, 405, 200, 389, 420, 407, 410, 411, 408, 407, 417, 416, 200, 200, 200, 200, 200, 200, 410, 424, 405, 411, 200, 200, 407, 200, 240, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 220, 200, 235, 241, 200, 200, 240, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 248, 200, 200, 230, 228, 234, 229, 224, 227, 239, 233, 233, 200, 248, 200, 200, 200, 200, 262, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 247, 200, 200, 200, 200, 200, 200, 200, 251, 200, 247, 251, 200, 200, 200, 246, 200, 200, 200, 200, 200, 244, 242, 200, 200, 200, 200, 250, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 247, 200, 200, 244, 244, 200, 200, 200\n  y sizes: 16\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_hdf('C:\\\\Users\\\\pablo\\\\Desktop\\\\TFG (1)\\\\test_datasets\\\\sequences_100\\\\sequences_2.h5', key='df2')\n",
    "\n",
    "# Extract sequences and labels\n",
    "seqs = df['sequences'].to_list()\n",
    "labels = df['label'].astype(np.uint8).to_numpy()\n",
    "\n",
    "# Print information about the data\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "print(f\"Number of sequences: {len(seqs)}\")\n",
    "print(f\"Shape of the first image in the first sequence: {seqs[0][0].shape}\")\n",
    "\n",
    "# Normalize image data to [0, 1] and ensure sequences are numpy arrays\n",
    "#seqs = np.array([np.array(sequence) / 255.0 for sequence in seqs])\n",
    "\n",
    "# Ensure the sequences have the correct shape (num_sequences, num_frames, height, width, channels)\n",
    "# Adjust the shape based on your image dimensions and channels (e.g., (20, 200, 200, 3) for RGB images)\n",
    "# In this example, let's assume the images are 200x200 and RGB\n",
    "#seqs = np.array([np.array(sequence).reshape(20, 200, 200, 3) for sequence in seqs])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(seqs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes to ensure everything is correct\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "for sequence in seqs:\n",
    "    print(len(sequence))\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Training labels size: {len(y_train)}\")\n",
    "print(f\"Testing labels size: {len(y_test)}\")\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# CNN part (to process each frame)\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(20, 200, 200, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# RNN part (to process sequences of frames)\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(seqs, labels, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepFakeDetector",
   "language": "python",
   "name": "deepfakedetector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
