{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D,MaxPooling3D, Conv3D, Flatten,Lambda, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from faceRecon import FaceExtractorMultithread, FaceExtractor\n",
    "\n",
    "route = '/home/pabloarga/Data'\n",
    "\n",
    "print('Loading dataframes...')\n",
    "fragments = []\n",
    "for i in range(5):\n",
    "    # Assuming each dataframe is stored as a separate table in the HDF5 file\n",
    "    if i%10==0:\n",
    "        print(f'{i} dataframes loaded')\n",
    "    chunk = pd.read_hdf(f'{route}/dataframe{i}_FaceForensics.h5', key=f'df{i}')\n",
    "    fragments.append(chunk)\n",
    "#fragments = [pd.read_hdf(f'dataframes/CelebDB/dataframe{i}_600videos.h5', key=f'df{i}') for i in range(2)]#6\n",
    "df = pd.concat(fragments)\n",
    "\n",
    "print(df.describe())\n",
    "print(df.dtypes)\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "X = df.drop(['label'], axis = 1)\n",
    "y = df['label']\n",
    "\n",
    "print('Dividing dataset into train and test...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify = y)\n",
    "X_train = np.stack(X_train['face'], axis=0)\n",
    "X_test = np.stack(X_test['face'], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating model...')\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(200, 200, 3)))\n",
    "model.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetricsModule import TrainingMetrics\n",
    "#prueba de que imprime las stats\n",
    "metrics = TrainingMetrics(model,\"/home/pabloarga/Results\")\n",
    "metrics.train(X_train, y_train, X_test, y_test, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de bach training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten,Lambda,  Input,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from MetricsModule import TrainingMetrics\n",
    "\n",
    "route =  'P:\\TFG\\Datasets\\dataframes_small' #'/home/pabloarga/Data' \n",
    "resultsPath = 'P:\\TFG\\Datasets\\dataframes_small\\\\results' #'/home/pabloarga/Results'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(200, 200, 3)))\n",
    "model.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Input(shape=(200, 200, 3)))\n",
    "model2.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.2))  # Dropout for regularization\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Input(shape=(200, 200, 3)))\n",
    "model3.add(Lambda(lambda x: x/255.0)) #normalizamos los valores de los pixeles -> mejora la eficiencia\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2)))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(0.2))  # Dropout for regularization\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dropout(0.3))  # Dropout for regularization\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with batch: 1/1\n",
      "Epoch 1/2\n",
      "130/130 [==============================] - 273s 2s/step - loss: 0.2284 - accuracy: 0.9505 - val_loss: 0.0494 - val_accuracy: 0.9913\n",
      "Epoch 2/2\n",
      "130/130 [==============================] - 278s 2s/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0622 - val_accuracy: 0.9923\n",
      "33/33 [==============================] - 51s 2s/step\n"
     ]
    }
   ],
   "source": [
    "from MetricsModule import TrainingMetrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "\n",
    "# Congelar las capas del modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crear un nuevo modelo encima del modelo base\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "metrics = TrainingMetrics(model, resultsPath)\n",
    "metrics.batches_train(route,nBatches = 1 , epochs = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos el modelo y probamos con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from FaceReconModule import FaceExtractorMultithread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "modelPath = os.path.join(resultsPath,'model.keras')\n",
    "\n",
    "#cargamos un video y lo procesamos frame por frame\n",
    "videoPath = 'P:\\TFG\\Datasets\\FaceForensics\\manipulated_sequences-fake\\DeepFakeDetection\\c23\\\\videos\\\\01_15__outside_talking_pan_laughing__02HILKYO.mp4'\n",
    "#creamos un dataframe con el path al video y con la label 0 de fake\n",
    "df = pd.DataFrame({'video': [videoPath], 'label': [0]})\n",
    "faceExtractor = FaceExtractorMultithread(400) #cada 2 frames\n",
    "imagesDataset = faceExtractor.transform(df)\n",
    "\n",
    "print(imagesDataset['face'])\n",
    "\n",
    "#Cargamos el modelo\n",
    "model = load_model(modelPath,safe_mode=False)\n",
    "#probamos el modelo manualmente\n",
    "y_pred = model.predict(np.stack(imagesDataset['face'], axis=0))\n",
    "#matriz de confusión\n",
    "y_real = imagesDataset['label']\n",
    "print(confusion_matrix(y_real, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D,MaxPooling3D, Conv3D, Flatten,Lambda, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from FaceReconModule import FaceExtractorMultithread, FaceExtractor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TODO comentar\n",
    "\"\"\"\n",
    "def augment(row):\n",
    "    image = np.array(row['face']).reshape((200, 200, 3))  # Replace height and width with the dimensions of your images\n",
    "\n",
    "    # Perform augmentations only on fake images\n",
    "    if row['label'] == 1:\n",
    "        # Flip the image\n",
    "        flipped_image = np.fliplr(image)\n",
    "        # Rotate the image\n",
    "        rotated_image = ndimage.rotate(image, 15)  # Adjust the angle as needed\n",
    "\n",
    "        # Add the augmented images as new examples\n",
    "        new_row_flipped = row.copy()\n",
    "        new_row_flipped['face'] = flipped_image.flatten().tolist()\n",
    "        new_row_flipped['label'] = 1\n",
    "        new_row_rotated = row.copy()\n",
    "        new_row_rotated['face'] = rotated_image.flatten().tolist()\n",
    "        new_row_rotated['label'] = 1\n",
    "\n",
    "        return pd.DataFrame([row, new_row_flipped, new_row_rotated])\n",
    "\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folderPath = 'E:\\TFG\\Datasets\\dataframes\\\\valid\\\\dataframes_combined'\n",
    "\n",
    "nBatches = 10\n",
    "\n",
    "numDataframes = len([name for name in os.listdir(folderPath) if os.path.isfile(os.path.join(folderPath, name))])\n",
    "#Calculamos el tamaño de cada fragmento\n",
    "fragmentSize = int(numDataframes/nBatches)\n",
    "\n",
    "for i in range(nBatches):\n",
    "    fragments = [pd.read_hdf(f'{folderPath}/dataframe{j}_FaceForensics.h5', key=f'df{j}') for j in range(fragmentSize*i,fragmentSize*(i+1))]\n",
    "    df = pd.concat(fragments)\n",
    "    #contamos el número de fakes y reales en el dataframe\n",
    "    print(df['label'].value_counts())\n",
    "    #print(\"After augmentation\")\n",
    "    #df = pd.concat(df.apply(augment, axis=1).tolist(), ignore_index=True)\n",
    "    #print(df['label'].value_counts())\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'conv2d_5' expected 2 variables, but received 0 variables during loading. Expected: ['conv2d_5/kernel:0', 'conv2d_5/bias:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m videoTest \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/pabloarga/testVideos/testVideo.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Cargamos el modelo\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#obtenemos el número de frames del video\u001b[39;00m\n\u001b[1;32m     11\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(videoTest)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_api.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following argument(s) are not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the native Keras format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:275\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    272\u001b[0m             asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:263\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     asset_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43massets_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masset_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m weights_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asset_store:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:456\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[1;32m    447\u001b[0m     _load_state(\n\u001b[1;32m    448\u001b[0m         child_obj,\n\u001b[1;32m    449\u001b[0m         weights_store,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m         visited_trackables\u001b[38;5;241m=\u001b[39mvisited_trackables,\n\u001b[1;32m    454\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[0;32m--> 456\u001b[0m     \u001b[43m_load_container_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43massets_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:513\u001b[0m, in \u001b[0;36m_load_container_state\u001b[0;34m(container, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     used_names[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 513\u001b[0m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrackable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43massets_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/saving/saving_lib.py:425\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001b[0m\n\u001b[1;32m    418\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    419\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load weights in object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrackable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    423\u001b[0m             )\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_own_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m assets_store:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_mismatch:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/base_layer.py:3539\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m   3537\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[1;32m   3538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[0;32m-> 3539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3544\u001b[0m     )\n\u001b[1;32m   3545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[1;32m   3546\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[1;32m   3547\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Layer 'conv2d_5' expected 2 variables, but received 0 variables during loading. Expected: ['conv2d_5/kernel:0', 'conv2d_5/bias:0']"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "\n",
    "#cargamos el modelo del escritorio\n",
    "modelPath = '/home/pabloarga/Results/2024-03-04 14.39.53/model2024-03-04 14.39.53.keras'\n",
    "videoTest = '/home/pabloarga/testVideos/testVideo.mp4'\n",
    "\n",
    "#Cargamos el modelo\n",
    "model = load_model(modelPath,safe_mode=False,compile=False)\n",
    "#obtenemos el número de frames del video\n",
    "cap = cv2.VideoCapture(videoTest)\n",
    "nFramesVideo = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#Procesamos el video frame por frame\n",
    "faceExtractor = FaceExtractorMultithread(nFramesVideo) #cada 2 frames\n",
    "frames = faceExtractor.process_video(videoTest,1)[0]\n",
    "\n",
    "#probamos el modelo manualmente\n",
    "y_pred = model.predict(np.stack(frames, axis=0))\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepFakeDetector",
   "language": "python",
   "name": "deepfakedetector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
